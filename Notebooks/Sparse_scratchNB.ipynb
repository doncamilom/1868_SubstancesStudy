{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as sp_linalg\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depending on the dataset (and the memory we have) you may use the following dtypes:\n",
    "\n",
    "- np.ubyte: 1 byte Unsigned char ( 0 ... 255 )\n",
    "- np.byte : 1 byte Signed char ( -128 ... 127 )\n",
    "- np.short: 2 bytes C short   ( -32,768 ... 32,767 )  <-- This is more than enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getVec_sparse_v2(tx,elemList): #Tx = molecular formula, e.g. C4H2ClBr\n",
    "    #### This regex handles non-integer subindices: C6H16Na3O12.5 (which happens in DS) \n",
    "    Li = re.split(r\"(?<!^)(?=[A-Z])\",tx)  #Split as ['H2','O']\n",
    "    \n",
    "    # Adds 1 if no subindex. Result is ['H2','O1']. \n",
    "    # Right after, split chem symbol from subindex as [['H',2],['O',1]]\n",
    "    \n",
    "    li = [re.split(r\"([A-z]+)(([0-9]*[.])?[0-9]+)\",i)\n",
    "          if bool(re.match(r'[A-z]*([0-9]*[.])?[0-9]+',i))\n",
    "          else re.split(r\"([A-z]+)(([0-9]*[.])?[0-9]+)\",i+'1') for i in Li]  \n",
    "    \n",
    "    # Construct two lists: input for sparse matrix construction\n",
    "    col  = [elemList.index(i[1]) for i in li]  # Index of element i to put correspondent data\n",
    "    data = [float(i[2]) for i in li]           # Num. atoms of element i\n",
    "    \n",
    "    for i in data:\n",
    "        if float(i)!=int(i):\n",
    "            return None  # Return empty lists or better None?\n",
    "    return col,data\n",
    "\n",
    "def getElems(DataFile,NMax=None):\n",
    "    \"\"\"NMax specifies number of rows of dataset to be used\"\"\"\n",
    "    col_names = ['ID','formula','year']\n",
    "    sep = '\\t'\n",
    "\n",
    "    df = pd.read_csv(DataFile,header=None,sep=sep,nrows=NMax,names=col_names)  #Load data\n",
    "    \n",
    "    df['formula'] = df['formula'].str.strip()   #Remove white spaces at begginning and end of string \n",
    "\n",
    "    elems = set([])\n",
    "\n",
    "    for cmpnd in df['formula']:\n",
    "        txt = ''.join(re.findall(r'[A-z]',cmpnd))   #Remove all subindices (there must be a regex to this but who knows)\n",
    "        elems = elems.union(  set(re.split(r\"(?<!^)(?=[A-Z])\",txt))  )  # Add elements of this set to the set of known elements\n",
    "\n",
    "    elems = sorted(list(elems)) # Convert to list and sort\n",
    "\n",
    "    # Save this list of elements so it doesn't have to be calculated every time\n",
    "    #with open(\"./Data/ElementList.txt\", \"w\") as f:\n",
    "    #    for A in elems:\n",
    "    #        f.write(str(A) +\"\\n\")\n",
    "\n",
    "    return elems  # This returns a list with all sorted elements in dataset\n",
    "\n",
    "def allVecs_sparse(DataFile,NMax=None):\n",
    "    col_names = ['ID','formula','year']\n",
    "    sep = '\\t'\n",
    "\n",
    "    df = pd.read_csv(DataFile,header=None,sep=sep,nrows=NMax,names=col_names)  #Load data\n",
    "        \n",
    "    df['formula'] = df['formula'].str.strip()   #Remove white spaces at begginning and end of string \n",
    "\n",
    "    elemList = getElems(DataFile,NMax)\n",
    "    \n",
    "    # List of lists [col,data]\n",
    "    colXdata = list(map(lambda x: getVec_sparse_v2(x,elemList) , df['formula'].values))\n",
    "    index = [i for i, l in enumerate(colXdata) if l is not None]\n",
    "    colXdata = [l for l in colXdata if l is not None]\n",
    "    \n",
    "    # See docs for scipy.sparse.csr_matrix to understand the syntaxis\n",
    "    indptr = np.cumsum([0]+list(map(lambda x: len(x[0]) , colXdata)))\n",
    "    indices = np.array(list(chain(*[l[0] for l in colXdata])))\n",
    "    data = np.array(list(chain(*[l[1] for l in colXdata])))\n",
    "\n",
    "    cmpnds = sp.csr_matrix((data, indices, indptr), \n",
    "                           shape=(len(colXdata), len(elemList)),\n",
    "                           dtype=np.short)\n",
    "       \n",
    "    years = df['year'].values[index]\n",
    "    subsID = df['ID'].values[index]\n",
    "    \n",
    "    del indptr, indices, data, df, index, colXdata\n",
    "\n",
    "    return cmpnds,years,subsID, elemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11138x60 sparse matrix of type '<class 'numpy.int16'>'\n",
       "\twith 41691 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse , _ , _1 , elemList = allVecs_sparse('../SendProfGR/sample_w_IDs.csv')\n",
    "sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.6 ns ± 1.47 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "a = sparse.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRs_sparse(cmpnds,elemList):\n",
    "    indices = cmpnds.indices\n",
    "    data = cmpnds.data\n",
    "    indptr = cmpnds.indptr\n",
    "    \n",
    "    sz_cols = cmpnds.shape[1] # Number of elements\n",
    "    \n",
    "    Rs = []  \n",
    "    cmpnd_num_Rs = []\n",
    "    \n",
    "    for c in range(indptr.shape[0]-1):\n",
    "        indx = indices[indptr[c]:indptr[c+1]]\n",
    "        sub_dat = data[indptr[c]:indptr[c+1]]\n",
    "        \n",
    "        for i in range(indx.shape[0]):\n",
    "            c_data = sub_dat.copy()\n",
    "            n = int(c_data[i])          \n",
    "            \n",
    "            for j in range(n):   #Loop through ith element's subindex\n",
    "                c_data[i] -= 1   #Remove one\n",
    "                n = j+1          #How many atoms of this element have been removed \n",
    "\n",
    "                #Append compound data with a reduced entry (R-X n-1)\n",
    "                Rs.append(  np.append(c_data.copy(),n)  )\n",
    "\n",
    "        cmpnd_num_Rs.append((sub_dat.sum(),indx))\n",
    "   \n",
    "    # Construct sparse matrix\n",
    "    for_indics = list(chain(*[l[0]*[l[1]] for l in cmpnd_num_Rs]) )\n",
    "\n",
    "    indptr = np.cumsum([0]+list(map(lambda x: len(x)+1 , for_indics)))\n",
    "    indices = np.array(list(chain(*[list(l)+[sz_cols] for l in for_indics])))\n",
    "    data = np.array(list(chain(*[l for l in Rs])))\n",
    "\n",
    "    Rs = sp.csr_matrix((data, indices, indptr),\n",
    "                        shape=(len(Rs), sz_cols+1),\n",
    "                        dtype=np.short)\n",
    "\n",
    "    return Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_cmpnds_arr = np.unique(sparse.toarray(),axis=0)\n",
    "uniq_sparse = sp.csr_matrix(uniq_cmpnds_arr,dtype=np.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.99 s, sys: 55.8 ms, total: 3.04 s\n",
      "Wall time: 3.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<211425x61 sparse matrix of type '<class 'numpy.int16'>'\n",
       "\twith 1102416 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Rs = findRs_sparse(uniq_sparse,elemList)\n",
    "Rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Next step: Finding which Rs are more than once in DS.\n",
    "\n",
    "1. Distribute the total list of Rs in chunks of non-overlapping sets\n",
    "    - That means, if a vector `v` is in chunk $i$, then all of its repetitions (if any) should as well be in chunk i and in no other chunk $j \\ne i$.\n",
    "    \n",
    "    - That way we can take advantage of multiple processors.\n",
    "\n",
    "\n",
    "    \n",
    "2. Find non-unique vectors within each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validRs(Rs):\n",
    "    \"\"\"Get all non-unique R-n vectors out of a chunk Rs.\"\"\"\n",
    "    new_rs , c = np.unique(Rs.toarray(),axis=0, return_counts=True)\n",
    "    new_rs = new_rs[c > 1]\n",
    "    return sp.csr_matrix(new_rs,dtype=np.short)\n",
    "\n",
    "def unique_mp(Rs,size=4):\n",
    "    \"\"\"Create data chunks for finding non-unique Rs in parallel.\n",
    "    Test how this does on full DS. If pickling errors, implement recursive spliting.\n",
    "    \"\"\"\n",
    "    max_n = 4\n",
    "    # First split by n, the most natural choice.\n",
    "    dist_list = [Rs[(Rs[:,-1]==i).toarray()[:,-1] ]\n",
    "                  for i in range(1,max_n)]   + [Rs[(Rs[:,-1]>=max_n).toarray()[:,-1]]]\n",
    "\n",
    "    with mp.Pool(processes=size) as pool:\n",
    "        R_results = [pool.apply_async(validRs,args=(r,))\n",
    "                     for r in dist_list]        \n",
    "\n",
    "        Rs_get = [r.get() for r in R_results]\n",
    "\n",
    "    return Rs_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<2715x61 sparse matrix of type '<class 'numpy.int16'>'\n",
       " \twith 11245 stored elements in Compressed Sparse Row format>,\n",
       " <1874x61 sparse matrix of type '<class 'numpy.int16'>'\n",
       " \twith 7781 stored elements in Compressed Sparse Row format>,\n",
       " <560x61 sparse matrix of type '<class 'numpy.int16'>'\n",
       " \twith 2150 stored elements in Compressed Sparse Row format>,\n",
       " <965x61 sparse matrix of type '<class 'numpy.int16'>'\n",
       " \twith 3759 stored elements in Compressed Sparse Row format>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqRs = unique_mp(Rs,size=4)\n",
    "uniqRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribRs_forUnique(Rs,max_n):\n",
    "\n",
    "    def split_chunk(chunk,i):\n",
    "        maxSplit = 5  # Maximum number of subsplits you want \n",
    "\n",
    "        # Split using ith index:\n",
    "        split = []\n",
    "        step = 2\n",
    "        for j in range(maxSplit):\n",
    "            lower,upper = j*step,(j+1)*step\n",
    "            if j < maxSplit-1:         tmp_splt = chunk[(chunk[:,i]>=lower) & (chunk[:,i]<upper)]  # Entries that are either j or j+1\n",
    "            else:                      tmp_splt = chunk[chunk[:,i]>=lower]   # Entries that are maxSplit-1 or larger\n",
    "            split.append(tmp_splt)\n",
    "\n",
    "        # Now recursively further split here\n",
    "        newList = []\n",
    "        for l in split:\n",
    "            if l.nbytes/1e6 > maxWeightArray:\n",
    "                print(\"\\t** Found a very large chunk! (Inside recursive function)\")\n",
    "                newList = newList + split_chunk(l,i+1)  # Further split l by next i\n",
    "            elif l.shape[0]>1:      newList.append(l)  # Append only if chunk contains more than one entry\n",
    "        #    else:   newList.append(l)  # Append only if chunk contains more than one entry\n",
    "\n",
    "        return newList\n",
    "\n",
    "\n",
    "    # First split by n, the most natural choice.\n",
    "    dis_list = [Rs[Rs[:,-1]==i] for i in range(1,max_n)]\n",
    "\n",
    "    print(\"\\nStarting recursive splitting of Rs\")\n",
    "\n",
    "    new_chunks = []\n",
    "    for l in dis_list:\n",
    "        if l.nbytes/1e6 > maxWeightArray:\n",
    "            print(\"\\t** Found a very large chunk!\")\n",
    "            new_chunks = new_chunks + split_chunk(l,0)\n",
    "        else:        new_chunks.append(l)\n",
    "\n",
    "    print(\"Ended recursion\")\n",
    "\n",
    "    return new_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6114, 61)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_cmpnds = np.unique(sparse.toarray(),axis=0)\n",
    "Rs = findRs(uniq_cmpnds)\n",
    "\n",
    "Rs,c=np.unique(Rs,axis=0,return_counts=True)\n",
    "Rs[c>1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
