{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as sp_linalg\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depending on the dataset (and the memory we have) you may use the following dtypes:\n",
    "\n",
    "- np.ubyte: 1 byte Unsigned char ( 0 ... 255 )\n",
    "- np.byte : 1 byte Signed char ( -128 ... 127 )\n",
    "- np.short: 2 bytes C short   ( -32,768 ... 32,767 )  <-- This is more than enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getVec_sparse_v2(tx,elemList): #Tx = molecular formula, e.g. C4H2ClBr\n",
    "    #### This regex handles non-integer subindices: C6H16Na3O12.5 (which happens in DS) \n",
    "    Li = re.split(r\"(?<!^)(?=[A-Z])\",tx)  #Split as ['H2','O']\n",
    "    \n",
    "    # Adds 1 if no subindex. Result is ['H2','O1']. \n",
    "    # Right after, split chem symbol from subindex as [['H',2],['O',1]]\n",
    "    \n",
    "    li = [re.split(r\"([A-z]+)(([0-9]*[.])?[0-9]+)\",i)\n",
    "          if bool(re.match(r'[A-z]*([0-9]*[.])?[0-9]+',i))\n",
    "          else re.split(r\"([A-z]+)(([0-9]*[.])?[0-9]+)\",i+'1') for i in Li]  \n",
    "    \n",
    "    # Construct two lists: input for sparse matrix construction\n",
    "    col  = [elemList.index(i[1]) for i in li]  # Index of element i to put correspondent data\n",
    "    data = [float(i[2]) for i in li]           # Num. atoms of element i\n",
    "    \n",
    "    for i in data:\n",
    "        if float(i)!=int(i):\n",
    "            return None  # Return empty lists or better None?\n",
    "    return col,data\n",
    "\n",
    "def getElems(DataFile,NMax=None):\n",
    "    \"\"\"NMax specifies number of rows of dataset to be used\"\"\"\n",
    "    col_names = ['ID','formula','year']\n",
    "    sep = '\\t'\n",
    "\n",
    "    df = pd.read_csv(DataFile,header=None,sep=sep,nrows=NMax,names=col_names)  #Load data\n",
    "    \n",
    "    df['formula'] = df['formula'].str.strip()   #Remove white spaces at begginning and end of string \n",
    "\n",
    "    elems = set([])\n",
    "\n",
    "    for cmpnd in df['formula']:\n",
    "        txt = ''.join(re.findall(r'[A-z]',cmpnd))   #Remove all subindices (there must be a regex to this but who knows)\n",
    "        elems = elems.union(  set(re.split(r\"(?<!^)(?=[A-Z])\",txt))  )  # Add elements of this set to the set of known elements\n",
    "\n",
    "    elems = sorted(list(elems)) # Convert to list and sort\n",
    "\n",
    "    # Save this list of elements so it doesn't have to be calculated every time\n",
    "    #with open(\"./Data/ElementList.txt\", \"w\") as f:\n",
    "    #    for A in elems:\n",
    "    #        f.write(str(A) +\"\\n\")\n",
    "\n",
    "    return elems  # This returns a list with all sorted elements in dataset\n",
    "\n",
    "def allVecs_sparse(DataFile,NMax=None):\n",
    "    col_names = ['ID','formula','year']\n",
    "    sep = '\\t'\n",
    "\n",
    "    df = pd.read_csv(DataFile,header=None,sep=sep,nrows=NMax,names=col_names)  #Load data\n",
    "        \n",
    "    df['formula'] = df['formula'].str.strip()   #Remove white spaces at begginning and end of string \n",
    "\n",
    "    elemList = getElems(DataFile,NMax)\n",
    "    \n",
    "    # List of lists [col,data]\n",
    "    colXdata = list(map(lambda x: getVec_sparse_v2(x,elemList) , df['formula'].values))\n",
    "    index = [i for i, l in enumerate(colXdata) if l is not None]\n",
    "    colXdata = [l for l in colXdata if l is not None]\n",
    "    \n",
    "    # See docs for scipy.sparse.csr_matrix to understand the syntaxis\n",
    "    indptr = np.cumsum([0]+list(map(lambda x: len(x[0]) , colXdata)))\n",
    "    indices = np.array(list(chain(*[l[0] for l in colXdata])))\n",
    "    data = np.array(list(chain(*[l[1] for l in colXdata])))\n",
    "\n",
    "    cmpnds = sp.csr_matrix((data, indices, indptr), \n",
    "                           shape=(len(colXdata), len(elemList)),\n",
    "                           dtype=np.short)\n",
    "       \n",
    "    years = df['year'].values[index]\n",
    "    subsID = df['ID'].values[index]\n",
    "    \n",
    "    del indptr, indices, data, df, index, colXdata\n",
    "\n",
    "    return cmpnds,years,subsID, elemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11138x60 sparse matrix of type '<class 'numpy.int16'>'\n",
       "\twith 41691 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse , years , subsID , elemList = allVecs_sparse('../SendProfGR/sample_w_IDs.csv')\n",
    "sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRs_sparse(cmpnds,elemList):\n",
    "    indices = cmpnds.indices\n",
    "    data = cmpnds.data\n",
    "    indptr = cmpnds.indptr\n",
    "    \n",
    "    sz_cols = cmpnds.shape[1] # Number of elements\n",
    "    \n",
    "    Rs = []  \n",
    "    cmpnd_num_Rs = []\n",
    "    \n",
    "    for c in range(indptr.shape[0]-1):\n",
    "        indx = indices[indptr[c]:indptr[c+1]]\n",
    "        sub_dat = data[indptr[c]:indptr[c+1]]\n",
    "        \n",
    "        for i in range(indx.shape[0]):\n",
    "            c_data = sub_dat.copy()\n",
    "            n = int(c_data[i])          \n",
    "            \n",
    "            for j in range(n):   #Loop through ith element's subindex\n",
    "                c_data[i] -= 1   #Remove one\n",
    "                n = j+1          #How many atoms of this element have been removed \n",
    "\n",
    "                #Append compound data with a reduced entry (R-X n-1)\n",
    "                Rs.append(  np.append(c_data.copy(),n)  )\n",
    "\n",
    "        cmpnd_num_Rs.append((sub_dat.sum(),indx))\n",
    "   \n",
    "    # Construct sparse matrix\n",
    "    for_indics = list(chain(*[l[0]*[l[1]] for l in cmpnd_num_Rs]) )\n",
    "\n",
    "    indptr = np.cumsum([0]+list(map(lambda x: len(x)+1 , for_indics)))\n",
    "    indices = np.array(list(chain(*[list(l)+[sz_cols] for l in for_indics])))\n",
    "    data = np.array(list(chain(*[l for l in Rs])))\n",
    "\n",
    "    Rs = sp.csr_matrix((data, indices, indptr),\n",
    "                        shape=(len(Rs), sz_cols+1),\n",
    "                        dtype=np.short)\n",
    "\n",
    "    return Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_cmpnds_arr = np.unique(sparse.toarray(),axis=0)\n",
    "uniq_sparse = sp.csr_matrix(uniq_cmpnds_arr,dtype=np.short)\n",
    "\n",
    "years , subsID = years[:uniq_sparse.shape[0]] , subsID[:uniq_sparse.shape[0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.25 s, sys: 182 ms, total: 4.43 s\n",
      "Wall time: 4.53 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<211425x61 sparse matrix of type '<class 'numpy.int16'>'\n",
       "\twith 1102416 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Rs = findRs_sparse(uniq_sparse,elemList)\n",
    "Rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Next step: Finding which Rs are more than once in DS.\n",
    "\n",
    "1. Distribute the total list of Rs in chunks of non-overlapping sets\n",
    "    - That means, if a vector `v` is in chunk $i$, then all of its repetitions (if any) should as well be in chunk i and in no other chunk $j \\ne i$.\n",
    "    \n",
    "    - That way we can take advantage of multiple processors.\n",
    "\n",
    "\n",
    "    \n",
    "2. Find non-unique vectors within each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validRs(Rs):\n",
    "    \"\"\"Get all non-unique R-n vectors out of a chunk Rs.\"\"\"\n",
    "    new_rs , c = np.unique(Rs.toarray(),axis=0, return_counts=True)\n",
    "    new_rs = new_rs[c > 1]\n",
    "    return sp.csr_matrix(new_rs,dtype=np.short)\n",
    "\n",
    "def unique_mp(Rs,size=4):\n",
    "    \"\"\"Create data chunks for finding non-unique Rs in parallel.\n",
    "    Test how this does on full DS. If pickling errors, implement recursive spliting.\n",
    "    \"\"\"\n",
    "    max_n = 4\n",
    "    # First split by n, the most natural choice.\n",
    "    dist_list = [Rs[(Rs[:,-1]==i).toarray()[:,-1] ]\n",
    "                  for i in range(1,max_n)]   + [Rs[(Rs[:,-1]>=max_n).toarray()[:,-1]]]\n",
    "\n",
    "    with mp.Pool(processes=size) as pool:\n",
    "        R_results = [pool.apply_async(validRs,args=(r,))\n",
    "                     for r in dist_list]        \n",
    "\n",
    "        Rs_get = [r.get() for r in R_results]\n",
    "\n",
    "    return Rs_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<2715x61 sparse matrix of type '<class 'numpy.int16'>'\n",
       " \twith 11245 stored elements in Compressed Sparse Row format>,\n",
       " <1874x61 sparse matrix of type '<class 'numpy.int16'>'\n",
       " \twith 7781 stored elements in Compressed Sparse Row format>,\n",
       " <560x61 sparse matrix of type '<class 'numpy.int16'>'\n",
       " \twith 2150 stored elements in Compressed Sparse Row format>,\n",
       " <965x61 sparse matrix of type '<class 'numpy.int16'>'\n",
       " \twith 3759 stored elements in Compressed Sparse Row format>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqRs = unique_mp(Rs,size=4)\n",
    "uniqRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "elemDict = {}\n",
    "for i,elem in enumerate(elemList):\n",
    "    elemDict[i] = elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0th R evaluated...\n",
      "\t1000th R evaluated...\n",
      "\t2000th R evaluated...\n",
      "\t0th R evaluated...\n",
      "\t1000th R evaluated...\n",
      "\t0th R evaluated...\n",
      "\t0th R evaluated...\n"
     ]
    }
   ],
   "source": [
    "def get_matches(Rs,cmpnds,years,subsID,elemDict):\n",
    "    \"\"\"Get matches. For each R(n) find all elements X such that compound R-Xn exists in dataset. \n",
    "    Build an element set for each R(n).\n",
    "    \"\"\"\n",
    "    ns = Rs[:,-1].data\n",
    "    R  = Rs[:,:-1]\n",
    "    \n",
    "    sumCmpnds = cmpnds.sum(axis=1)\n",
    "    sumRaxis1 = np.array(    R.sum(axis=1).flatten() + ns    ).flatten()\n",
    "    \n",
    "    Matches = []\n",
    "    for i,n in enumerate(ns):\n",
    "        if i%1000==0:       print( f\"\\t{i}th R evaluated...\" )\n",
    "            \n",
    "        r = R[i] #The actual R\n",
    "        \n",
    "        \"\"\"Encode a condition to search only within a subset of compounds\n",
    "        fulfulling certain conditions based on R\"\"\"\n",
    "        # 1. R is contained in compound        \n",
    "        cond1 = ((cmpnds - r.toarray())>=0).all(axis=1)\n",
    "        # 2. sum of atoms in cmpnd == sum of atoms in R_ (sum(R) + n)\n",
    "        cond2 = (sumCmpnds == sumRaxis1[i])\n",
    "        \n",
    "        cond = np.array(cond1 & cond2).flatten()  # Combine conditions\n",
    "        subsetCmpnds = cmpnds[cond]  # Select subset of cmpnds\n",
    "        curr_years = years[cond]\n",
    "        curr_subsID = subsID[cond]\n",
    "        \n",
    "        cmpnds_no_R = (subsetCmpnds - r.toarray())\n",
    "        \n",
    "        # Now select only those cmpnds where residual is due to one element only (X_n)\n",
    "        # Only useful for n!=1\n",
    "        if n!=1:\n",
    "            cond = np.array((cmpnds_no_R!=0).sum(axis=1)==1).flatten()\n",
    "            subsetCmpnds = subsetCmpnds[cond]\n",
    "            curr_years = curr_years[cond]\n",
    "            curr_subsID = curr_subsID[cond]\n",
    "            \n",
    "        # At this point, subsetCmpnds contains all compounds that match with R(n).\n",
    "        elemIndex = (subsetCmpnds - r.toarray()).nonzero()[1]\n",
    "        curr_list = set(map(lambda x: elemDict[x]  , elemIndex))  # Map dict to above list of elems\n",
    "\n",
    "        Matches.append( [curr_list, curr_years, curr_subsID] )\n",
    "\n",
    "        \n",
    "        \n",
    "        ###########\n",
    "        ## Deal with this after you get all data created above\n",
    "        #Table_list.append(getTable(curr_list,curr_years,curr_subsID,useID=useID))      \n",
    "        ###########\n",
    "    return Matches      # [list_elems,list_years, list_ids] for each R(n)\n",
    "    \n",
    "for i in range(4): get_matches(uniqRs[i],uniq_sparse,years,subsID,elemDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommonal(Rs,rank,cmpnds,years,subsID,elemDict,useID=False):\n",
    "    \"\"\"Get Commonalities. For each R(n) find all elements X such that compound R-Xn exists in dataset. \n",
    "    Build a list of these for each R(n)\n",
    "    rank is the number of processors in which the operation is to be run\"\"\"\n",
    "\n",
    "    \n",
    "    if rank==0:   print(\"\\n Finding commonalities (finding sets for each (R,n) pair)...\\n\")\n",
    "    \n",
    "    \n",
    "    Comm_list = []\n",
    "    R_list = []\n",
    "    Table_list = []\n",
    "    # Now generate the R representations on TP (TPR).\n",
    "\n",
    "    j=0 #Counter for Rs with more than one appearence\n",
    "    sumCmpnds = cmpnds.sum(axis=1)\n",
    "\n",
    "    for i,R_ in enumerate(Rs):\n",
    "        if i%1000==0 and rank==0:       print( f\"\\t{i}th R evaluated...\" )\n",
    "\n",
    "        n = R_[-1]  #Take subindex \n",
    "        R = R_[:-1] #The actual R\n",
    "        curr_list = []\n",
    "\n",
    "        # Encode a condition to search only within a subset of compounds fulfulling certain conditions based on R\n",
    "        # 1. R is contained in compound\n",
    "        cond1 = ((cmpnds - R) >= 0).all(axis=1)\n",
    "        # 2. sum of atoms in cmpnd == sum of atoms in R_ (sum(R) + n)\n",
    "        cond2 = (cmpnds.sum(axis=1) == R_.sum())\n",
    "\n",
    "        subsetCmpnds = cmpnds[cond1 & cond2]  # Select subset of cmpnds\n",
    "        curr_years = years[cond1 & cond2]\n",
    "        curr_subsID = subsID[cond1 & cond2]\n",
    "\n",
    "        cmpnds_no_R = (subsetCmpnds - R)\n",
    "        # Now select only those cmpnds where residual is due to one element only (X_n)\n",
    "        subsetCmpnds = subsetCmpnds[(cmpnds_no_R!=0).sum(axis=1)==1]\n",
    "        curr_years = years[cond1 & cond2]\n",
    "        curr_subsID = subsID[cond1 & cond2]\n",
    "\n",
    "        cmpnds_no_R = (subsetCmpnds - R)\n",
    "        # Now select only those cmpnds where residual is due to one element only (X_n)\n",
    "        subsetCmpnds = subsetCmpnds[(cmpnds_no_R!=0).sum(axis=1)==1]\n",
    "        curr_years = curr_years[(cmpnds_no_R!=0).sum(axis=1)==1]\n",
    "        curr_subsID = curr_subsID[(cmpnds_no_R!=0).sum(axis=1)==1]\n",
    "\n",
    "        if subsetCmpnds.shape[0] > 1:\n",
    "\n",
    "            curr_list = list(map(lambda x: elemDict[x]  , (subsetCmpnds - R).nonzero()[1] ))\n",
    "            Table_list.append(getTable(curr_list,curr_years,curr_subsID,useID=useID))\n",
    "            Comm_list.append(curr_list)\n",
    "            R_list.append(R_)\n",
    "            j+=1\n",
    "\n",
    "    if rank==0:    print(\"Saving...\")\n",
    "    table_list_arr = np.array(Table_list,dtype=np.intc)\n",
    "    np.save(f'./Data/TablesID_NMax{NMax}_P{rank}.npy',table_list_arr)\n",
    "    np.save(f'./Data/RVector_NMax{NMax}_P{rank}.npy',np.array(R_list,dtype=np.short))\n",
    "\n",
    "\n",
    "    ### Make a function here that produces a new array exchanging ID for year, just a mapping\n",
    "    if useID:\n",
    "        if rank==0:  print(\"\\n\\tProducing array filled with years from ID array...\")\n",
    "\n",
    "        # Mapping going from ID to year\n",
    "        mapping = dict(zip(list(subsID),list(years)))\n",
    "        mapping[-1] = -1\n",
    "        # Apply mapping \n",
    "        yearArray = np.vectorize(mapping.__getitem__)(table_list_arr)\n",
    "        np.save(f'./Data/TablesYears_NMax{NMax}_P{rank}.npy',np.array(yearArray,dtype=np.short))\n",
    "\n",
    "    return Comm_list,R_list  #This list contains lists (one for each R) of elements X such that R-X exist in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribRs_forUnique(Rs,max_n):\n",
    "\n",
    "    def split_chunk(chunk,i):\n",
    "        maxSplit = 5  # Maximum number of subsplits you want \n",
    "\n",
    "        # Split using ith index:\n",
    "        split = []\n",
    "        step = 2\n",
    "        for j in range(maxSplit):\n",
    "            lower,upper = j*step,(j+1)*step\n",
    "            if j < maxSplit-1:         tmp_splt = chunk[(chunk[:,i]>=lower) & (chunk[:,i]<upper)]  # Entries that are either j or j+1\n",
    "            else:                      tmp_splt = chunk[chunk[:,i]>=lower]   # Entries that are maxSplit-1 or larger\n",
    "            split.append(tmp_splt)\n",
    "\n",
    "        # Now recursively further split here\n",
    "        newList = []\n",
    "        for l in split:\n",
    "            if l.nbytes/1e6 > maxWeightArray:\n",
    "                print(\"\\t** Found a very large chunk! (Inside recursive function)\")\n",
    "                newList = newList + split_chunk(l,i+1)  # Further split l by next i\n",
    "            elif l.shape[0]>1:      newList.append(l)  # Append only if chunk contains more than one entry\n",
    "        #    else:   newList.append(l)  # Append only if chunk contains more than one entry\n",
    "\n",
    "        return newList\n",
    "\n",
    "\n",
    "    # First split by n, the most natural choice.\n",
    "    dis_list = [Rs[Rs[:,-1]==i] for i in range(1,max_n)]\n",
    "\n",
    "    print(\"\\nStarting recursive splitting of Rs\")\n",
    "\n",
    "    new_chunks = []\n",
    "    for l in dis_list:\n",
    "        if l.nbytes/1e6 > maxWeightArray:\n",
    "            print(\"\\t** Found a very large chunk!\")\n",
    "            new_chunks = new_chunks + split_chunk(l,0)\n",
    "        else:        new_chunks.append(l)\n",
    "\n",
    "    print(\"Ended recursion\")\n",
    "\n",
    "    return new_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6114, 61)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_cmpnds = np.unique(sparse.toarray(),axis=0)\n",
    "Rs = findRs(uniq_cmpnds)\n",
    "\n",
    "Rs,c=np.unique(Rs,axis=0,return_counts=True)\n",
    "Rs[c>1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oldSciPy",
   "language": "python",
   "name": "oldscipy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
